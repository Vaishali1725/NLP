{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nc2R0dkDqtlU"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH4QHzpmtCm7"
   },
   "source": [
    "## SENTENCE SEGMENTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXOU-mFSt2W1",
    "outputId": "b55492e8-8e90-4401-c0d0-67b5c5dcdfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Steve is a food lover.', 'Steve loves Chinese.', 'Mexican is his second favourite Cuisine.']\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\" Steve is a food lover. Steve loves Chinese. Mexican is his second favourite Cuisine. \"\"\"\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ88kD1iuMZF"
   },
   "source": [
    "## WORD TOKENISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOjlZ-TruMvC",
    "outputId": "9bfd5f82-c6db-48a0-f36b-a119cea7df8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Steve', 'is', 'a', 'food', 'lover', '.']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentences[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvEt_l_0zH-C"
   },
   "source": [
    "## BASICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1z-0ogNsBhb"
   },
   "source": [
    "### STOP WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "07nIfm6OsJzk"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R00yZKyYscCG",
    "outputId": "33cdf4d2-4a28-4340-cf8c-0ce51422a77d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcwDhpBjse6s",
    "outputId": "1736e039-e283-4683-fae5-c1a45c2f993e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "N66uqGV4uoaK"
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"You were there. I was there. We were there. Together in a way that we hadn't been for a long time, In a way that we won't be again.\n",
    "Fluorescent lights, it's warm. Yellow skies, and brown sunflowers. My mind is hazy, no thoughts ... just silence. I'm with you. \n",
    "Your Honey hues, soft skin, warm eyes were enchanting in a way the same way. You laugh, there's no sound. \n",
    "You feel like home, familiar and filled with sweetness. Your lips began to move, why can't I hear? Honey spills from your lips. \n",
    "You've become a hive, overfilling and spilling out the brim.  The Golden liquid pools around my ankles, I bask in it. I want this; I want you. \n",
    "You invite me into your arms, and I run towards you,  the warm thickness envelopes me as I embrace you.  The honey covers us. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUBPWPocuyko",
    "outputId": "61c93abd-b3ee-4562-dc88-f5fff6622981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You were there.', 'I was there.', 'We were there.', \"Together in a way that we hadn't been for a long time, In a way that we won't be again.\", \"Fluorescent lights, it's warm.\", 'Yellow skies, and brown sunflowers.', 'My mind is hazy, no thoughts ... just silence.', \"I'm with you.\", 'Your Honey hues, soft skin, warm eyes were enchanting in a way the same way.', \"You laugh, there's no sound.\", 'You feel like home, familiar and filled with sweetness.', \"Your lips began to move, why can't I hear?\", 'Honey spills from your lips.', \"You've become a hive, overfilling and spilling out the brim.\", 'The Golden liquid pools around my ankles, I bask in it.', 'I want this; I want you.', 'You invite me into your arms, and I run towards you,  the warm thickness envelopes me as I embrace you.', 'The honey covers us.']\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3ZuB2kIufko"
   },
   "source": [
    "### STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJ0PzJkEu6Dc",
    "outputId": "106cab22-eb65-41b9-a93c-90476a343619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you .\n",
      "___________________________________________________________\n",
      "i .\n",
      "___________________________________________________________\n",
      "we .\n",
      "___________________________________________________________\n",
      "togeth way n't long time , in way wo n't .\n",
      "___________________________________________________________\n",
      "fluoresc light , 's warm .\n",
      "___________________________________________________________\n",
      "yellow sky , brown sunflow .\n",
      "___________________________________________________________\n",
      "my mind hazi , thought ... silenc .\n",
      "___________________________________________________________\n",
      "i 'm .\n",
      "___________________________________________________________\n",
      "your honey hue , soft skin , warm eye enchant way way .\n",
      "___________________________________________________________\n",
      "you laugh , 's sound .\n",
      "___________________________________________________________\n",
      "you feel like home , familiar fill sweet .\n",
      "___________________________________________________________\n",
      "your lip began move , ca n't i hear ?\n",
      "___________________________________________________________\n",
      "honey spill lip .\n",
      "___________________________________________________________\n",
      "you 've becom hive , overfil spill brim .\n",
      "___________________________________________________________\n",
      "the golden liquid pool around ankl , i bask .\n",
      "___________________________________________________________\n",
      "i want ; i want .\n",
      "___________________________________________________________\n",
      "you invit arm , i run toward , warm thick envelop i embrac .\n",
      "___________________________________________________________\n",
      "the honey cover us .\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "    print(sentences[i])\n",
    "    print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJOujpTqvchS"
   },
   "source": [
    "### LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrl8jWAovOQV",
    "outputId": "74757382-5b90-4cde-8c21-9a2ae18cbdd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You .\n",
      "___________________________________________________________\n",
      "I .\n",
      "___________________________________________________________\n",
      "We .\n",
      "___________________________________________________________\n",
      "Together way n't long time , In way wo n't .\n",
      "___________________________________________________________\n",
      "Fluorescent light , 's warm .\n",
      "___________________________________________________________\n",
      "Yellow sky , brown sunflower .\n",
      "___________________________________________________________\n",
      "My mind hazy , thought ... silence .\n",
      "___________________________________________________________\n",
      "I 'm .\n",
      "___________________________________________________________\n",
      "Your Honey hue , soft skin , warm eye enchanting way way .\n",
      "___________________________________________________________\n",
      "You laugh , 's sound .\n",
      "___________________________________________________________\n",
      "You feel like home , familiar filled sweetness .\n",
      "___________________________________________________________\n",
      "Your lip began move , ca n't I hear ?\n",
      "___________________________________________________________\n",
      "Honey spill lip .\n",
      "___________________________________________________________\n",
      "You 've become hive , overfilling spilling brim .\n",
      "___________________________________________________________\n",
      "The Golden liquid pool around ankle , I bask .\n",
      "___________________________________________________________\n",
      "I want ; I want .\n",
      "___________________________________________________________\n",
      "You invite arm , I run towards , warm thickness envelope I embrace .\n",
      "___________________________________________________________\n",
      "The honey cover u .\n",
      "___________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "    print(sentences[i])\n",
    "    print(\"___________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eH9R1RsRwMYV"
   },
   "source": [
    "## WAY OF REPRESENTING TEXT DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05f4PrZRwdL3",
    "outputId": "2719e93d-872a-4437-cae3-88edd7408049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Tom love Stephanie.', 'Jerry love Bob.', 'Bob, Stephanie and Tom love Jerry.', \"Tom don't look good during eating.\"]\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"\"\" Tom love Stephanie. Jerry love Bob. Bob, Stephanie and Tom love Jerry. Tom don't look good during eating. \"\"\"\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXZPxxSwwm9L",
    "outputId": "ddd91939-e041-482c-d7dc-0b9e896f1ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom love stephani']\n",
      "_______________________________________________________________________________________\n",
      "['tom love stephani', 'jerri love bob']\n",
      "_______________________________________________________________________________________\n",
      "['tom love stephani', 'jerri love bob', 'bob stephani tom love jerri']\n",
      "_______________________________________________________________________________________\n",
      "['tom love stephani', 'jerri love bob', 'bob stephani tom love jerri', 'tom look good eat']\n",
      "_______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    #review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "    print(corpus)\n",
    "    print(\"_______________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQuvnO7RyQVX"
   },
   "source": [
    "### BAG OF WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wHRv_BBHw2AN",
    "outputId": "a0966d3b-5e42-4ab0-a09f-54005a2c15c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 1 1]\n",
      " [1 0 0 1 0 1 0 0]\n",
      " [1 0 0 1 0 1 1 1]\n",
      " [0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0p6axQlyU_3"
   },
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC6kmAOYw4ja",
    "outputId": "59e55441-641f-4fe8-d6ab-95c5cad5e745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.53256952\n",
      "  0.65782931 0.53256952]\n",
      " [0.61366674 0.         0.         0.61366674 0.         0.49681612\n",
      "  0.         0.        ]\n",
      " [0.48163503 0.         0.         0.48163503 0.         0.38992506\n",
      "  0.48163503 0.38992506]\n",
      " [0.         0.5417361  0.5417361  0.         0.5417361  0.\n",
      "  0.         0.34578314]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "OH4QHzpmtCm7",
    "wJ88kD1iuMZF",
    "v3ZuB2kIufko",
    "TJOujpTqvchS",
    "eH9R1RsRwMYV"
   ],
   "name": "LOLz.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
