{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem=[\"cats\", \"cacti\", \"geese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats', 'cacti', 'geese']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats:cat\n",
      "cacti:cactus\n",
      "geese:goose\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_stem:\n",
    "    print(i + \":\" + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos\n",
    "peace=\"What do you mean, 'I don't believe in God'? I talk to him everyday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peace_tokenize = word_tokenize(peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP')]\n",
      "[('do', 'VB')]\n",
      "[('you', 'PRP')]\n",
      "[('mean', 'NN')]\n",
      "[(',', ',')]\n",
      "[(\"'\", \"''\")]\n",
      "[('I', 'PRP')]\n",
      "[('do', 'VB')]\n",
      "[(\"n't\", 'RB')]\n",
      "[('believe', 'VB')]\n",
      "[('in', 'IN')]\n",
      "[('God', 'NNP')]\n",
      "[(\"'\", \"''\")]\n",
      "[('?', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('talk', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('him', 'PRP')]\n",
      "[('everyday', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in peace_tokenize:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#named entity recognition\n",
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "john = 'John lives in New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "john_token=word_tokenize(john)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'NNP'),\n",
       " ('lives', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('New', 'NNP'),\n",
       " ('York', 'NNP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "john_tags=nltk.pos_tag(john_token)\n",
    "john_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (PERSON John/NNP) lives/VBZ in/IN (GPE New/NNP York/NNP))\n"
     ]
    }
   ],
   "source": [
    "john_ner=ne_chunk(john_tags)\n",
    "print(john_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is sparta!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "sparta\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparta"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = doc[2]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparta!!"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[2:5]\n",
    "span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 This\n",
      "1 is\n",
      "2 sparta\n",
      "3 !\n",
      "4 !\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"This is sparta!!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.i,token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joey doesn't share pizza!!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"Joey doesn't share pizza!!\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Joey PROPN\n",
      "1 does AUX\n",
      "2 n't PART\n",
      "3 share VERB\n",
      "4 pizza NOUN\n",
      "5 ! PUNCT\n",
      "6 ! PUNCT\n"
     ]
    }
   ],
   "source": [
    "#pos tagging\n",
    "for token in doc:\n",
    "    print(token.i,token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ner\n",
    "doc = nlp(\"Apple is looking to buy U.K. startup for 1$ billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "1$ billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Barack Obama the former president of United States will be vacating white house today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama PERSON\n",
      "United States GPE\n",
      "today DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matcher\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'the', 'former', 'president', 'of', 'United', 'States', 'will', 'be', 'vacating', 'white', 'house', 'today']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Barack Obama the former president of United States will be vacating white house today\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "#matcher = Matcher(nlp.vocab)\n",
    "#pattern =[{'LEMME':'vacate'},{'ORTH':'white'}]\n",
    "#matcher.add(\"vacate_pattern\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = doc[start:end]  \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id,start,end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"2018 FIFA world cup : France won!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=[{'IS_DIGIT':True},{'LOWER':'fifa'},{'LOWER':'world'},{'LOWER':'cup'}]\n",
    "matcher2=Matcher(nlp.vocab)\n",
    "matcher2.add('fifa_pattern',[pattern])\n",
    "matcher2=matcher2(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 FIFA world cup\n"
     ]
    }
   ],
   "source": [
    "for match_id,start,end in matcher2:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
